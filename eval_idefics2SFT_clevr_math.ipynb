{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the following paths to the appropriate paths for your system\n",
    "\n",
    "args = {\n",
    "    'model_name_or_path': '/home/mila/s/sarvjeet-singh.ghotra/scratch/git/rl_ft_vlm_tools/pre_trained_models/idefics2-8b',\n",
    "    'use_quant': True,\n",
    "    'train_file': '/network/projects/aishwarya_lab/datasets/clevr-math/',\n",
    "    'eval_batch_size': 1,\n",
    "    'num_workers': 8,\n",
    "    'val_idx_file': 'val_idx_800.csv',\n",
    "    'test_idx_file': 'test_idx_800.csv',\n",
    "    'eval_output_file': 'outputs/evals/idefics2_eval_800.json',\n",
    "    'test_output_file': 'outputs/evals/idefics2_test_800.csv',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_TO_ID = {\n",
    "    'subtraction': 0,\n",
    "    'addition': 1,\n",
    "    'adversarial': 2,\n",
    "    'subtraction-multihop': 3\n",
    "}\n",
    "\n",
    "ID_TO_TEMPLATE = {\n",
    "    0: 'subtraction',\n",
    "    1: 'addition',\n",
    "    2: 'adversarial',\n",
    "    3: 'subtraction-multihop'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "        args['model_name_or_path'],\n",
    "        do_image_splitting=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import Idefics2ForConditionalGeneration\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "\n",
    "model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "    args['model_name_or_path'],\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=bnb_config if args['use_quant'] else None,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval code\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "def evaluate_generation(args, model, dataset, dataloader, processor, tokenizer, output_file):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    templates = []\n",
    "    idxs = []\n",
    "    for i, batch in tqdm(enumerate(dataloader), total=len(dataloader), desc='Evaluation Gen Loop'):\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "        batch_templates = batch['templates']\n",
    "        batch_idxs = batch['idxs']\n",
    "        # remove 'templates' from batch\n",
    "        batch.pop('templates')\n",
    "        batch.pop('idxs')\n",
    "        batch.pop('answers')\n",
    "        output_ = model.generate(\n",
    "                        **batch,\n",
    "                        # max_length=args['max_input_length'],\n",
    "                        # output_scores=True,\n",
    "                        return_dict_in_generate=True,\n",
    "                        num_beams=1,\n",
    "                        use_cache=True,\n",
    "                        do_sample=False,\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                        eos_token_id=tokenizer.eos_token_id,\n",
    "                        max_new_tokens=128,\n",
    "                    )\n",
    "\n",
    "        generated_ids = output_.sequences\n",
    "        # generated_ids = pad_across_processes(generated_ids, dim=1, pad_index=tokenizer.pad_token_id, pad_first=True)\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # print(\"=======================\")\n",
    "        # print(generated_texts)\n",
    "        # print(\"=======================\")\n",
    "        # print()\n",
    "\n",
    "        pred_ans = [x.split(\"Assistant: \")[1][:-1].strip() for x in generated_texts]    # -1 removes \".\"\n",
    "        # pred_ans = pred_ans.strip()\n",
    "\n",
    "        # preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True).strip() for g in generated_ids]\n",
    "        predictions.extend(pred_ans)\n",
    "        # target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True).strip() for t in labels]\n",
    "        targets.extend(labels)\n",
    "        templates.extend(batch_templates)\n",
    "        idxs.extend(batch_idxs)\n",
    "        \n",
    "    # predictions = predictions[:len(dataset)]\n",
    "    # targets = targets[:len(dataset)]\n",
    "    # templates = templates[:len(dataset)]\n",
    "\n",
    "    results = []\n",
    "    corr = 0\n",
    "    total = 0\n",
    "    temp_based_acc = {}\n",
    "    \n",
    "    for pred, tar, temp, idx in zip(predictions, targets, templates, idxs):\n",
    "        tar = str(tar.detach().cpu().numpy())\n",
    "        cur_res = {\n",
    "            'pred': pred,\n",
    "            'target': tar,\n",
    "            'template': ID_TO_TEMPLATE[int(temp)],\n",
    "            'idx': int(idx.detach().cpu().numpy())\n",
    "        }\n",
    "        results.append(cur_res)\n",
    "\n",
    "        temp = str(temp.detach().cpu().numpy())\n",
    "        if temp not in temp_based_acc:\n",
    "            temp_based_acc[temp] = {}\n",
    "            temp_based_acc[temp]['corr'] = 0\n",
    "            temp_based_acc[temp]['total'] = 0\n",
    "        \n",
    "        if pred == tar:\n",
    "            corr += 1\n",
    "            temp_based_acc[temp]['corr'] += 1\n",
    "        total += 1\n",
    "        temp_based_acc[temp]['total'] += 1\n",
    "\n",
    "    # save first before execute to trace error.\n",
    "    res_path = output_file\n",
    "    print(f\"Saving results to {res_path}\")\n",
    "    with open(res_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    # if args['wandb_log']:\n",
    "    #     table = wandb.Table(dataframe=pd.DataFrame(results))\n",
    "    #     wandb.log({\"predictions\": table})\n",
    "\n",
    "    value_accuracy = corr / len(results) * 100\n",
    "    print(f\"[Eval Info] value_accuracy: {value_accuracy:.5g}%\")\n",
    "    # value_accuracy = torch.FloatTensor([value_accuracy]).to(accelerator.device)\n",
    "\n",
    "    print(temp_based_acc)\n",
    "\n",
    "    templates_value_accuracy = {}\n",
    "    for temp, acc in temp_based_acc.items():\n",
    "        acc = acc['corr'] / acc['total'] * 100\n",
    "        temp_name = ID_TO_TEMPLATE[int(temp)]\n",
    "        print(f\"[Eval Info] value_accuracy on {temp_name} category: {acc:.5g}%\")\n",
    "        # templates_value_accuracy[temp_name] = torch.FloatTensor([acc]).to(accelerator.device)\n",
    "    \n",
    "    # Metric summary:\n",
    "    out_stats = {'value_accuracy': value_accuracy}\n",
    "    for temp_category, acc in templates_value_accuracy.items():\n",
    "        out_stats[temp_category] = acc\n",
    "\n",
    "    # model.train()\n",
    "    return out_stats\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class MyDataCollatorFewShots:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "        self.image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
    "            processor.tokenizer.additional_special_tokens.index(\"<image>\")\n",
    "        ]\n",
    "        self.few_shots_txt = []\n",
    "        self.few_shots_img = [Image.open(\"outputs/add_2.png\").convert(\"RGB\"),\n",
    "                              Image.open(\"outputs/sub_1.png\").convert(\"RGB\"),\n",
    "                              Image.open(\"outputs/adv_1.png\").convert(\"RGB\"),\n",
    "                              Image.open(\"outputs/sub_multihop_1.png\").convert(\"RGB\")]\n",
    "        # {\"type\": \"image\"},\n",
    "        # {\"type\": \"text\", \"text\": \"Final answer = Total small matte blocks + 1 (small matte block).\\nHow many small matte blocks are there? 2.\\nAdd 1 small matte block to total small matte blocks: 2 + 1 = 3.\\nFinal answer: 3. \\nExample 2: \"}, # Add\n",
    "        # {\"type\": \"image\"},\n",
    "        # {\"type\": \"text\", \"text\": \"Final answer = Total cylinders - Total cyan cylinders.\\nFind total cylinders: 2.\\nFind cyan cylinders: 0.\\nSubtract cyan cylinders from total cylinders: 2 - 0 = 2.\\nFinal answer: 2.\\n Example 4: \"}, # Adv.\n",
    "        # {\"type\": \"image\"},\n",
    "        # {\"type\": \"text\", \"text\": \"Final answer = Total objects - Total matte blocks - Total blue matte objects.\\nFind total objects: 7.\\nFind total matte blocks: 1.\\nFind total blue matte objects: 1.\\nSubtract matte blocks and blue matte objects from total objects: 7 - 1 - 1 = 5.\\nFinal answer: 5. \\nExample 5: \"}, # Sub. Multihop\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        texts = []\n",
    "        images = []\n",
    "        answers = []\n",
    "        temps = []\n",
    "        for example in examples:\n",
    "            image = example[\"image\"]\n",
    "            question = example[\"query\"]\n",
    "            answer = example[\"answer\"]\n",
    "            # messages = [\n",
    "            #     f\"<image>From the image, {question} Answer: \",\n",
    "            # ]\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Answer briefly.\"},\n",
    "                        {\"type\": \"image\"},\n",
    "                        {\"type\": \"text\", \"text\": {question}},\n",
    "                    ]\n",
    "                },\n",
    "            ]\n",
    "            #     {\n",
    "            #         \"role\": \"assistant\",\n",
    "            #         \"content\": [\n",
    "            #         {\"type\": \"text\", \"text\": \"Solution steps:\\nAnswer = Total blocks - Total big cyan blocks.\\nHow many blocks are there? 4.\\nHow many big blocks are there? 2.\\nHow many big blocks are there that are of cyan color? 1.\\nSubtract 1 big cyan block from all the blocks: 4 - 1 = 3.\\nFinal answer: 3.\"},\n",
    "            #         ]\n",
    "            #     },\n",
    "            #     {\n",
    "            #         \"role\": \"user\",\n",
    "            #         \"content\": [\n",
    "            #             {\"type\": \"image\"},\n",
    "            #             {\"type\": \"text\", \"text\": question + \"\\nSolution steps:\\nAnswer =\"}\n",
    "            #         ]\n",
    "            #     },\n",
    "            # ]\n",
    "            \n",
    "            # if example[\"split\"] == \"train\":\n",
    "            #     messages.append(\n",
    "            #         {\n",
    "            #             \"role\": \"assistant\",\n",
    "            #             \"content\": [\n",
    "            #                 {\"type\": \"text\", \"text\": answer}\n",
    "            #             ]\n",
    "            #         }\n",
    "            #     )\n",
    "\n",
    "            text = self.processor.apply_chat_template(messages, add_generation_prompt=False if example[\"split\"] == \"train\" else True)\n",
    "            texts.append(text.strip())  # Orig: texts.append(messages)\n",
    "            images.append([image])\n",
    "            answers.append(answer)\n",
    "\n",
    "            if examples[0]['split'] != 'train':\n",
    "                temps.append(TEMPLATE_TO_ID[example[\"template\"]])\n",
    "\n",
    "        batch = self.processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "        ans_tokens = torch.IntTensor(answers)\n",
    "\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = self.image_token_id\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"answers\"] = ans_tokens\n",
    "        batch['idxs'] = torch.IntTensor([x['idx'] for x in examples])\n",
    "\n",
    "        if examples[0]['split'] != 'train':\n",
    "            batch[\"labels\"] = ans_tokens\n",
    "            batch[\"templates\"] = torch.IntTensor(temps)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "# class MyDataCollator:\n",
    "#     def __init__(self, processor):\n",
    "#         self.processor = processor\n",
    "#         self.image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
    "#             processor.tokenizer.additional_special_tokens.index(\"<image>\")\n",
    "#         ]\n",
    "\n",
    "#     def __call__(self, examples):\n",
    "#         texts = []\n",
    "#         images = []\n",
    "#         answers = []\n",
    "#         temps = []\n",
    "#         for example in examples:\n",
    "#             image = example[\"image\"]\n",
    "#             question = example[\"query\"]\n",
    "#             answer = example[\"answer\"]\n",
    "#             messages = [\n",
    "#                 {\n",
    "#                     \"role\": \"user\",\n",
    "#                     \"content\": [\n",
    "#                         {\"type\": \"text\", \"text\": \"Answer briefly.\"},\n",
    "#                         {\"type\": \"image\"},\n",
    "#                         {\"type\": \"text\", \"text\": question}\n",
    "#                     ]\n",
    "#                 }\n",
    "#             ]\n",
    "#             if example[\"split\"] == \"train\":\n",
    "#                 messages.append(\n",
    "#                     {\n",
    "#                         \"role\": \"assistant\",\n",
    "#                         \"content\": [\n",
    "#                             {\"type\": \"text\", \"text\": answer}\n",
    "#                         ]\n",
    "#                     }\n",
    "#                 )\n",
    "#             text = self.processor.apply_chat_template(messages, add_generation_prompt=False if example[\"split\"] == \"train\" else True)\n",
    "#             texts.append(text.strip())\n",
    "#             images.append([image])\n",
    "#             answers.append(answer)\n",
    "\n",
    "#             if examples[0]['split'] != 'train':\n",
    "#                 temps.append(TEMPLATE_TO_ID[example[\"template\"]])\n",
    "\n",
    "#         batch = self.processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "#         ans_tokens = torch.IntTensor(answers)\n",
    "\n",
    "#         labels = batch[\"input_ids\"].clone()\n",
    "#         labels[labels == self.processor.tokenizer.pad_token_id] = self.image_token_id\n",
    "#         batch[\"labels\"] = labels\n",
    "#         batch[\"answers\"] = ans_tokens\n",
    "#         batch['idxs'] = torch.IntTensor([x['idx'] for x in examples])\n",
    "\n",
    "#         if examples[0]['split'] != 'train':\n",
    "#             batch[\"labels\"] = ans_tokens\n",
    "#             batch[\"templates\"] = torch.IntTensor(temps)\n",
    "\n",
    "#         return batch\n",
    "\n",
    "\n",
    "class ClverMathDataset(Dataset):\n",
    "    def __init__(self, data_dir, split, idx_file=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.dataset = load_dataset('dali-does/clevr-math',\n",
    "                               cache_dir=data_dir,\n",
    "                               split=split)\n",
    "        self.split = split.lower()\n",
    "        if idx_file is not None:\n",
    "            with open(idx_file, 'r') as f:\n",
    "                self.indices = f.read().splitlines()\n",
    "        else:\n",
    "            self.indices = list(range(len(self.dataset)))\n",
    "\n",
    "        self.indices = [int(x) for x in self.indices]\n",
    "        self.len = len(self.indices)\n",
    "\n",
    "        # self.len = len(self.dataset)\n",
    "        # if self.split != 'train':\n",
    "        #     # self.dataset = self.dataset[:100]\n",
    "        #     self.len = 2400\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.indices[idx]\n",
    "        ques = self.dataset[idx]['question']\n",
    "        label = self.dataset[idx]['label']\n",
    "        img = self.dataset[idx]['image'].convert('RGB')\n",
    "        template = self.dataset[idx]['template']\n",
    "        dp = {\n",
    "            'query': ques,\n",
    "            'image': img,\n",
    "            'answer': label,\n",
    "            'split': self.split,\n",
    "            'idx': idx,\n",
    "        }\n",
    "        if self.split != 'train':\n",
    "            dp['template'] = template\n",
    "        return dp\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "def prepare_datasets_and_data_loaders_idefics2(args, processor):\n",
    "    data_collator = MyDataCollatorFewShots(processor)\n",
    "\n",
    "    val_dataset = ClverMathDataset(args['train_file'], 'validation', args['val_idx_file'])\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args['eval_batch_size'],\n",
    "        collate_fn=data_collator,\n",
    "        num_workers=args['num_workers'],\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    test_dataset = ClverMathDataset(args['train_file'], 'test', args['test_idx_file'])\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args['eval_batch_size'],\n",
    "        collate_fn=data_collator,\n",
    "        num_workers=args['num_workers'],\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    return val_dataloader, test_dataloader\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dl, test_dl = prepare_datasets_and_data_loaders_idefics2(args, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Gen Loop:   0%|          | 0/801 [00:00<?, ?it/s]\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "Evaluation Gen Loop: 100%|██████████| 801/801 [03:08<00:00,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to outputs/evals/idefics2_eval_800.json\n",
      "[Eval Info] value_accuracy: 94.632%\n",
      "{'2': {'corr': 86, 'total': 89}, '1': {'corr': 270, 'total': 282}, '3': {'corr': 80, 'total': 92}, '0': {'corr': 322, 'total': 338}}\n",
      "[Eval Info] value_accuracy on adversarial category: 96.629%\n",
      "[Eval Info] value_accuracy on addition category: 95.745%\n",
      "[Eval Info] value_accuracy on subtraction-multihop category: 86.957%\n",
      "[Eval Info] value_accuracy on subtraction category: 95.266%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'value_accuracy': 94.63171036204744}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_generation(args, model, None, val_dl, processor, processor.tokenizer, args['eval_output_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fewshots iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dl, test_dl = prepare_datasets_and_data_loaders_idefics2(args, processor)\n",
    "example = val_dl.dataset[1]\n",
    "orig_query = example['query']\n",
    "print(orig_query)\n",
    "# example['query'] = \"From the image, subtract all blue balls and all gray blocks. How many balls are left in the image?\"\n",
    "# example['query'] = \"From the image, subtract all cylinders. How many objects are left?\"\n",
    "example['query'] = \"How many cylinders are there in the image?\"\n",
    "print(\"Example: \", example)\n",
    "few_shot_example = {\n",
    "    # 'query': \"From the image, subtract all balls. How many objects are left? Answer: 6.\",\n",
    "    'query': \"How many spheres are there in the image? Answer: 3.\",\n",
    "    'image': Image.open(\"outputs/tmp.png\").convert(\"RGB\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "images = []\n",
    "answers = []\n",
    "\n",
    "image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
    "            processor.tokenizer.additional_special_tokens.index(\"<image>\")\n",
    "        ]\n",
    "\n",
    "image = example[\"image\"]\n",
    "question = example[\"query\"]\n",
    "answer = example[\"answer\"]\n",
    "messages = [\n",
    "    f\"<image>{few_shot_example['query']}<image>{question} Answer:\",\n",
    "]\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\"type\": \"text\", \"text\": \"Think step by step.\"},\n",
    "#             {\"type\": \"image\"},\n",
    "#             {\"type\": \"text\", \"text\": \"Subtract all big cyan blocks. How many blocks are left?\"},\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         \"role\": \"assistant\",\n",
    "#         \"content\": [\n",
    "#         {\"type\": \"text\", \"text\": \"Solution steps:\\nAnswer = Total blocks - Total big cyan blocks.\\nHow many blocks are there? 4.\\nHow many big blocks are there? 2.\\nHow many big blocks are there that are of cyan color? 1.\\nSubtract 1 big cyan block from all the blocks: 4 - 1 = 3.\\nFinal answer: 3.\"},\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\"type\": \"image\"},\n",
    "#             {\"type\": \"text\", \"text\": question + \"\\nSolution steps:\\nAnswer =\"}\n",
    "#         ]\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# if example[\"split\"] == \"train\":\n",
    "#     messages.append(\n",
    "#         {\n",
    "#             \"role\": \"assistant\",\n",
    "#             \"content\": [\n",
    "#                 {\"type\": \"text\", \"text\": answer}\n",
    "#             ]\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "# text = self.processor.apply_chat_template(messages, add_generation_prompt=False if example[\"split\"] == \"train\" else True)\n",
    "texts.extend(messages)  # Orig: texts.append(messages)\n",
    "images.append([few_shot_example['image'], image])\n",
    "answers.append(answer)\n",
    "\n",
    "# if examples[0]['split'] != 'train':\n",
    "#     temps.append(TEMPLATE_TO_ID[example[\"template\"]])\n",
    "\n",
    "batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "ans_tokens = torch.IntTensor(answers)\n",
    "\n",
    "labels = batch[\"input_ids\"].clone()\n",
    "labels[labels == processor.tokenizer.pad_token_id] = image_token_id\n",
    "# batch[\"labels\"] = labels\n",
    "# batch[\"answers\"] = ans_tokens\n",
    "# batch['idxs'] = torch.IntTensor([x['idx'] for x in examples])\n",
    "\n",
    "# if examples[0]['split'] != 'train':\n",
    "#     batch[\"labels\"] = ans_tokens\n",
    "#     batch[\"templates\"] = torch.IntTensor(temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ = model.generate(\n",
    "                        **batch,\n",
    "                        # max_length=args['max_input_length'],\n",
    "                        # output_scores=True,\n",
    "                        return_dict_in_generate=True,\n",
    "                        num_beams=1,\n",
    "                        use_cache=True,\n",
    "                        do_sample=False,\n",
    "                        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                        max_new_tokens=64,\n",
    "                    )\n",
    "\n",
    "generated_ids = output_.sequences\n",
    "# generated_ids = pad_across_processes(generated_ids, dim=1, pad_index=tokenizer.pad_token_id, pad_first=True)\n",
    "# labels = batch['labels']\n",
    "\n",
    "generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"=======================\")\n",
    "print(generated_texts)\n",
    "print(\"=======================\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_len = 119202\n",
    "test_len = 7955\n",
    "sample_size = 800\n",
    "\n",
    "idx = list(range(val_len))\n",
    "import random\n",
    "random.shuffle(idx)\n",
    "\n",
    "val_idx = idx[:sample_size]\n",
    "test_idx = idx[:sample_size]\n",
    "\n",
    "# save idices to a csv file\n",
    "import pandas as pd\n",
    "val_df = pd.DataFrame(val_idx)\n",
    "test_df = pd.DataFrame(test_idx)\n",
    "val_df.to_csv(f'val_idx_{sample_size}.csv', index=False)\n",
    "test_df.to_csv(f'test_idx_{sample_size}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 800\n",
    "# [Eval Info] value_accuracy: 95.63%\n",
    "# {'2': {'corr': 86, 'total': 89}, '1': {'corr': 275, 'total': 282}, '3': {'corr': 79, 'total': 92}, '0': {'corr': 326, 'total': 338}}\n",
    "# [Eval Info] value_accuracy on adversarial category: 96.629%\n",
    "# [Eval Info] value_accuracy on addition category: 97.518%\n",
    "# [Eval Info] value_accuracy on subtraction-multihop category: 85.87%\n",
    "# [Eval Info] value_accuracy on subtraction category: 96.45%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idefics2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
